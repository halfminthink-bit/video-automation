"""
ç”»åƒã‚¿ã‚¤ãƒŸãƒ³ã‚°ãƒãƒƒãƒãƒ£ãƒ¼ï¼ˆLLMé§†å‹•å‹ï¼‰

Claude 3 Haikuã‚’ä½¿ç”¨ã—ã¦ã€æ–‡è„ˆã‚’ç†è§£ã—ãŸè‡ªç„¶ãªç”»åƒåˆ‡ã‚Šæ›¿ãˆã‚’å®Ÿç¾ã™ã‚‹ã€‚
LLMãŒæŒ‡å®šã—ãªã‹ã£ãŸéš™é–“æ™‚é–“ã«ã¯ã€Œæœªä½¿ç”¨ç”»åƒã€ã‚’è‡ªå‹•çš„ã«åŸ‹ã‚ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãªæŒ™å‹•ã‚’æä¾›ã€‚
"""

import json
import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False


class ImageTimingMatcherLLM:
    """
    LLMé§†å‹•å‹ç”»åƒã‚¿ã‚¤ãƒŸãƒ³ã‚°ãƒãƒƒãƒãƒ£ãƒ¼
    
    æ©Ÿèƒ½:
    - Claude 3 Haikuã‚’ä½¿ç”¨ã—ãŸæ–‡è„ˆç†è§£ã«åŸºã¥ãç”»åƒé…ç½®
    - ã‚»ã‚¯ã‚·ãƒ§ãƒ³å˜ä½ã§ã®APIå•ã„åˆã‚ã›ï¼ˆå‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å›é¿ï¼‰
    - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆå‰Šæ¸›
    - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é…ç½®ï¼ˆLLMæŒ‡å®š + éš™é–“åŸ‹ã‚ï¼‰
    """
    
    def __init__(
        self,
        working_dir: Path,
        api_key: Optional[str] = None,
        model: str = "claude-3-haiku-20240307",
        cache_dir: Optional[Path] = None,
        min_duration: float = 3.0,
        max_duration: float = 15.0,
        gap_threshold: float = 2.0,
        logger: Optional[logging.Logger] = None
    ):
        """
        åˆæœŸåŒ–
        
        Args:
            working_dir: ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            api_key: Anthropic APIã‚­ãƒ¼ï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            model: ä½¿ç”¨ã™ã‚‹Claudeãƒ¢ãƒ‡ãƒ«
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å…ˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: working_dir/07_compositionï¼‰
            min_duration: æœ€å°è¡¨ç¤ºæ™‚é–“ï¼ˆç§’ï¼‰
            max_duration: æœ€å¤§è¡¨ç¤ºæ™‚é–“ï¼ˆç§’ï¼‰
            gap_threshold: éš™é–“åŸ‹ã‚ã®é–¾å€¤ï¼ˆç§’ï¼‰
            logger: ãƒ­ã‚¬ãƒ¼
        """
        if not ANTHROPIC_AVAILABLE:
            raise ImportError(
                "anthropic package is required. Install with: pip install anthropic"
            )
        
        self.working_dir = Path(working_dir)
        self.model = model
        self.min_duration = min_duration
        self.max_duration = max_duration
        self.gap_threshold = gap_threshold
        self.logger = logger or logging.getLogger(__name__)
        
        # APIã‚­ãƒ¼ã®å–å¾—
        if api_key is None:
            import os
            api_key = os.getenv("CLAUDE_API_KEY")
            if not api_key:
                raise ValueError(
                    "CLAUDE_API_KEY environment variable is required. "
                    "Set it or pass api_key parameter."
                )
        
        self.api_client = Anthropic(api_key=api_key)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®šï¼ˆPhase 07ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼‰
        if cache_dir is None:
            # working_dir/07_composition/ ã«ä¿å­˜
            cache_dir = self.working_dir / "07_composition"
        else:
            cache_dir = Path(cache_dir)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file = self.cache_dir / "llm_allocation_cache.json"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®èª­ã¿è¾¼ã¿
        self.cache = self._load_cache()
    
    def _load_cache(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.warning(f"Failed to load cache: {e}")
                return {}
        return {}
    
    def _save_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.warning(f"Failed to save cache: {e}")
    
    def _get_cache_key(
        self,
        section_id: int,
        section_subtitles: List[Dict[str, Any]],
        section_images: List[Dict[str, Any]]
    ) -> str:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            section_id: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ID
            section_subtitles: ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®å­—å¹•ãƒªã‚¹ãƒˆ
            section_images: ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ç”»åƒãƒªã‚¹ãƒˆ
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ï¼ˆæ–‡å­—åˆ—ï¼‰
        """
        # å­—å¹•ã¨ç”»åƒã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        subtitle_hash = hashlib.md5(
            json.dumps(section_subtitles, sort_keys=True).encode('utf-8')
        ).hexdigest()[:8]
        
        image_hash = hashlib.md5(
            json.dumps(
                [(img.get('file_path', ''), img.get('keywords', [])) 
                 for img in section_images],
                sort_keys=True
            ).encode('utf-8')
        ).hexdigest()[:8]
        
        return f"{section_id}_{subtitle_hash}_{image_hash}"
    
    def match_images_to_subtitles(
        self,
        script_data: dict,
        classified_images: dict,
        subtitle_timing: List[dict],
        section_id: int
    ) -> List[Dict[str, Any]]:
        """
        å­—å¹•ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã«åŸºã¥ã„ã¦ç”»åƒã‚’ãƒãƒƒãƒãƒ³ã‚°ï¼ˆLLMé§†å‹•ï¼‰
        
        Args:
            script_data: å°æœ¬ãƒ‡ãƒ¼ã‚¿ï¼ˆscript.jsonï¼‰
            classified_images: åˆ†é¡æ¸ˆã¿ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆclassified.jsonï¼‰
            subtitle_timing: å­—å¹•ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ï¼ˆsubtitle_timing.jsonï¼‰
            section_id: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ID
            
        Returns:
            ç”»åƒã‚¯ãƒªãƒƒãƒ—ã®ãƒªã‚¹ãƒˆ
        """
        self.logger.info(f"ğŸ¤– LLM Image Timing Matcher initialized for Section {section_id}")
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å¢ƒç•Œã‚’å–å¾—ï¼ˆaudio_timing.jsonã‹ã‚‰ï¼‰
        section_boundaries = self._load_section_boundaries()
        if section_id not in section_boundaries:
            self.logger.warning(
                f"Section {section_id} boundaries not found. "
                "Falling back to subtitle-based calculation."
            )
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å­—å¹•ã‹ã‚‰è¨ˆç®—
            section_subtitles = [
                sub for sub in subtitle_timing
                if self._get_subtitle_section(sub, script_data) == section_id
            ]
            if not section_subtitles:
                self.logger.warning(f"No subtitles found for Section {section_id}")
                return []
            section_start = min(sub['start_time'] for sub in section_subtitles)
            section_end = max(sub['end_time'] for sub in section_subtitles)
        else:
            section_start, section_end = section_boundaries[section_id]
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®å­—å¹•ã‚’å–å¾—
        section_subtitles = [
            sub for sub in subtitle_timing
            if section_start <= sub['start_time'] < section_end
        ]
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ç”»åƒã‚’å–å¾—
        section_images = self._get_section_images(classified_images, section_id)
        
        self.logger.info(
            f"Section {section_id} ({section_start:.1f}s - {section_end:.1f}s): "
            f"Found {len(section_subtitles)} subtitles, {len(section_images)} images"
        )
        
        if not section_subtitles:
            self.logger.warning(f"No subtitles found for Section {section_id}")
            return self._create_fallback_clips(section_images, section_start, section_end)
        
        if not section_images:
            self.logger.warning(f"No images found for Section {section_id}")
            return []
        
        # LLMã«å•ã„åˆã‚ã›ã¦ç”»åƒé…ç½®ã‚’å–å¾—
        try:
            llm_allocations = self._get_allocations_from_llm(
                section_id,
                section_subtitles,
                section_images
            )
        except Exception as e:
            self.logger.error(f"LLM allocation failed: {e}. Falling back to equal split.")
            return self._create_fallback_clips(section_images, section_start, section_end)
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é…ç½®ãƒ­ã‚¸ãƒƒã‚¯ã‚’é©ç”¨
        image_clips = self._apply_allocations_and_fill_gaps(
            llm_allocations,
            section_subtitles,
            section_images,
            section_start,
            section_end
        )
        
        # ãƒ­ã‚°å‡ºåŠ›
        if image_clips:
            avg_duration = sum(
                clip['end_time'] - clip['start_time']
                for clip in image_clips
            ) / len(image_clips)
            self.logger.info(
                f"Section {section_id}: {len(image_clips)} image clips, "
                f"avg duration: {avg_duration:.1f}s"
            )
        
        return image_clips
    
    def _get_allocations_from_llm(
        self,
        section_id: int,
        section_subtitles: List[Dict[str, Any]],
        section_images: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        LLMã«å•ã„åˆã‚ã›ã¦ç”»åƒé…ç½®ã‚’å–å¾—
        
        Args:
            section_id: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ID
            section_subtitles: ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®å­—å¹•ãƒªã‚¹ãƒˆ
            section_images: ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ç”»åƒãƒªã‚¹ãƒˆ
            
        Returns:
            LLMãŒè¿”ã—ãŸé…ç½®ãƒªã‚¹ãƒˆ: [{"subtitle_id": 5, "image": "A.png"}, ...]
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        cache_key = self._get_cache_key(section_id, section_subtitles, section_images)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç¢ºèª
        if cache_key in self.cache:
            self.logger.info(f"âœ“ Using cached allocation for Section {section_id}")
            return self.cache[cache_key]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        prompt = self._build_prompt(section_subtitles, section_images)
        
        # LLMã«å•ã„åˆã‚ã›
        self.logger.info(f"ğŸ¤– Querying LLM for Section {section_id}...")
        try:
            response = self.api_client.messages.create(
                model=self.model,
                max_tokens=4096,
                system="You are a video director. Output valid JSON only. Do not include any explanatory text.",
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            response_text = response.content[0].text.strip()
            
            # JSONã‚’ãƒ‘ãƒ¼ã‚¹
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚’è€ƒæ…®
            if response_text.startswith("```"):
                # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
                lines = response_text.split("\n")
                response_text = "\n".join(lines[1:-1]) if len(lines) > 2 else response_text
            
            allocations = json.loads(response_text)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.cache[cache_key] = allocations
            self._save_cache()
            
            self.logger.info(f"âœ“ LLM allocation received: {len(allocations)} assignments")
            return allocations
            
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse LLM response as JSON: {e}")
            self.logger.debug(f"Response text: {response_text[:500]}")
            raise
        except Exception as e:
            self.logger.error(f"LLM API error: {e}")
            raise
    
    def _build_prompt(
        self,
        subtitles: List[Dict[str, Any]],
        images: List[Dict[str, Any]]
    ) -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        
        Args:
            subtitles: å­—å¹•ãƒªã‚¹ãƒˆ
            images: ç”»åƒãƒªã‚¹ãƒˆ
            
        Returns:
            ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        """
        # å­—å¹•ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
        subtitle_lines = []
        for i, sub in enumerate(subtitles):
            text_line1 = sub.get('text_line1', '').strip()
            text_line2 = sub.get('text_line2', '').strip()
            text = f"{text_line1} {text_line2}".strip()
            start_time = sub.get('start_time', 0.0)
            subtitle_lines.append(
                f"{i}. ID: {sub.get('index', i)}, Time: {start_time:.2f}s, Text: {text}"
            )
        
        # ç”»åƒãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
        image_lines = []
        for i, img in enumerate(images):
            file_path = Path(img.get('file_path', ''))
            filename = file_path.name
            keywords = img.get('keywords', [])
            source = img.get('source', 'unknown')
            image_lines.append(
                f"{i+1}. File: {filename} | Keywords: {keywords} | Source: {source}"
            )
        
        prompt = f"""You are a video director selecting images for a video section.

[Subtitle List]
{chr(10).join(subtitle_lines)}

[Image List]
{chr(10).join(image_lines)}

Task: Select the most appropriate image for each subtitle based on context and keywords.

Output format (JSON array):
[
  {{"subtitle_id": <subtitle_index>, "image": "<filename>"}},
  ...
]

Rules:
1. Match images to subtitles based on semantic relevance and keywords
2. You don't need to assign an image to every subtitle
3. Prioritize images that match the content of the subtitle text
4. Output only valid JSON, no explanations"""
        
        return prompt
    
    def _apply_allocations_and_fill_gaps(
        self,
        llm_allocations: List[Dict[str, Any]],
        section_subtitles: List[Dict[str, Any]],
        section_images: List[Dict[str, Any]],
        section_start: float,
        section_end: float
    ) -> List[Dict[str, Any]]:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é…ç½®ãƒ­ã‚¸ãƒƒã‚¯: LLMæŒ‡å®š + éš™é–“åŸ‹ã‚
        
        Args:
            llm_allocations: LLMãŒè¿”ã—ãŸé…ç½®ãƒªã‚¹ãƒˆ
            section_subtitles: ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®å­—å¹•ãƒªã‚¹ãƒˆ
            section_images: ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ç”»åƒãƒªã‚¹ãƒˆ
            section_start: ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹æ™‚é–“
            section_end: ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚é–“
            
        Returns:
            ç”»åƒã‚¯ãƒªãƒƒãƒ—ã®ãƒªã‚¹ãƒˆ
        """
        # Step 1: LLMæŒ‡å®šã®é…ç½®
        image_clips = []
        used_images = set()
        
        # å­—å¹•ID -> å­—å¹•ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        subtitle_map = {sub.get('index', i): sub for i, sub in enumerate(section_subtitles)}
        
        # LLMæŒ‡å®šã‚’å‡¦ç†
        for allocation in llm_allocations:
            subtitle_id = allocation.get('subtitle_id')
            image_filename = allocation.get('image')
            
            if subtitle_id not in subtitle_map:
                self.logger.warning(f"Subtitle ID {subtitle_id} not found, skipping")
                continue
            
            subtitle = subtitle_map[subtitle_id]
            start_time = subtitle.get('start_time', 0.0)
            end_time = subtitle.get('end_time', start_time + 3.0)
            
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            image_data = None
            for img in section_images:
                if Path(img.get('file_path', '')).name == image_filename:
                    image_data = img
                    used_images.add(image_filename)
                    break
            
            if image_data is None:
                self.logger.warning(f"Image {image_filename} not found, skipping")
                continue
            
            # ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
            image_clips.append({
                'image_path': str(Path(image_data.get('file_path', ''))),
                'start_time': start_time,
                'end_time': end_time,
                'keyword_matched': None,  # LLMé¸æŠãªã®ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã¯ãªã—
                'confidence': 1.0,
                'match_type': 'llm'
            })
        
        # ã‚¯ãƒªãƒƒãƒ—ã‚’é–‹å§‹æ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
        image_clips.sort(key=lambda x: x['start_time'])
        
        # Step 2: æœªä½¿ç”¨ç”»åƒã®ç‰¹å®š
        unused_images = [
            img for img in section_images
            if Path(img.get('file_path', '')).name not in used_images
        ]
        
        # Step 3: éš™é–“ï¼ˆGapsï¼‰ã®ç‰¹å®š
        gaps = []
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹ã‹ã‚‰æœ€åˆã®ç”»åƒã¾ã§
        if image_clips:
            first_clip_start = image_clips[0]['start_time']
            if first_clip_start - section_start >= self.gap_threshold:
                gaps.append((section_start, first_clip_start))
        else:
            # LLMæŒ‡å®šãŒ1ã¤ã‚‚ãªã„å ´åˆã¯å…¨ä½“ã‚’éš™é–“ã¨ã—ã¦æ‰±ã†
            gaps.append((section_start, section_end))
        
        # ç”»åƒé–“ã®éš™é–“
        for i in range(len(image_clips) - 1):
            current_end = image_clips[i]['end_time']
            next_start = image_clips[i + 1]['start_time']
            gap_duration = next_start - current_end
            if gap_duration >= self.gap_threshold:
                gaps.append((current_end, next_start))
        
        # æœ€å¾Œã®ç”»åƒã‹ã‚‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ‚äº†ã¾ã§
        if image_clips:
            last_clip_end = image_clips[-1]['end_time']
            if section_end - last_clip_end >= self.gap_threshold:
                gaps.append((last_clip_end, section_end))
        
        # Step 4: éš™é–“åŸ‹ã‚ï¼ˆé•·ã„éš™é–“ã®ã¿ã€å¾®ç´°ãªéš™é–“ã¯å¾Œã§å‰ã®ç”»åƒã‚’å»¶é•·ã—ã¦åŸ‹ã‚ã‚‹ï¼‰
        unused_image_index = 0
        for gap_start, gap_end in gaps:
            gap_duration = gap_end - gap_start
            
            # å¾®ç´°ãªéš™é–“ï¼ˆgap_thresholdæœªæº€ï¼‰ã¯å¾Œã§å‰ã®ç”»åƒã‚’å»¶é•·ã—ã¦åŸ‹ã‚ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚¹ã‚­ãƒƒãƒ—
            if gap_duration < self.gap_threshold:
                self.logger.debug(
                    f"Skipping small gap ({gap_duration:.3f}s < {self.gap_threshold}s). "
                    "Will be filled by extending previous image."
                )
                continue
            
            # æœ€å°è¡¨ç¤ºæ™‚é–“æœªæº€ã®éš™é–“ã‚‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¾Œã§å‰ã®ç”»åƒã‚’å»¶é•·ï¼‰
            if gap_duration < self.min_duration:
                self.logger.debug(
                    f"Skipping gap ({gap_duration:.3f}s < min_duration {self.min_duration}s). "
                    "Will be filled by extending previous image."
                )
                continue
            
            # é•·ã„éš™é–“ï¼ˆgap_thresholdä»¥ä¸Šï¼‰ã¯æ–°ã—ã„ç”»åƒã§åŸ‹ã‚ã‚‹
            # æœªä½¿ç”¨ç”»åƒãŒã‚ã‚Œã°ä½¿ç”¨ã€ãªã‘ã‚Œã°æ—¢å‡ºç”»åƒã‚’å†åˆ©ç”¨
            if unused_image_index < len(unused_images):
                image_data = unused_images[unused_image_index]
                unused_image_index += 1
            elif image_clips:
                # æœ€å¾Œã«ä½¿ç”¨ã—ãŸç”»åƒä»¥å¤–ã‚’é¸æŠ
                last_image = image_clips[-1]['image_path']
                for img in section_images:
                    if str(Path(img.get('file_path', ''))) != last_image:
                        image_data = img
                        break
                else:
                    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€å¾Œã®ç”»åƒã‚’å†åˆ©ç”¨
                    image_data = section_images[0]
            else:
                # ç”»åƒãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¾Œã§å‰ã®ç”»åƒã‚’å»¶é•·ï¼‰
                continue
            
            # éš™é–“ã‚’åŸ‹ã‚ã‚‹ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
            image_clips.append({
                'image_path': str(Path(image_data.get('file_path', ''))),
                'start_time': gap_start,
                'end_time': gap_end,
                'keyword_matched': None,
                'confidence': 0.0,
                'match_type': 'gap_fill'
            })
        
        # å†åº¦ã‚½ãƒ¼ãƒˆ
        image_clips.sort(key=lambda x: x['start_time'])
        
        # æ™‚é–“åˆ¶ç´„ã‚’é©ç”¨ï¼ˆé€£ç¶šæ€§ã‚’å®Œå…¨ã«ä¿è¨¼ï¼‰
        image_clips = self._apply_time_constraints(
            image_clips, 
            section_start, 
            section_end
        )
        
        return image_clips
    
    def _apply_time_constraints(
        self,
        clips: List[Dict[str, Any]],
        section_start: float,
        section_end: float
    ) -> List[Dict[str, Any]]:
        """
        æ™‚é–“åˆ¶ç´„ã‚’é©ç”¨ã—ã€é€£ç¶šæ€§ã‚’å®Œå…¨ã«ä¿è¨¼ï¼ˆAnti-Desync Logicï¼‰
        
        Args:
            clips: ç”»åƒã‚¯ãƒªãƒƒãƒ—ã®ãƒªã‚¹ãƒˆ
            section_start: ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹æ™‚é–“
            section_end: ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚é–“
            
        Returns:
            åˆ¶ç´„é©ç”¨å¾Œã®ã‚¯ãƒªãƒƒãƒ—ãƒªã‚¹ãƒˆï¼ˆéš™é–“ã‚¼ãƒ­ã€å®Œå…¨é€£ç¶šï¼‰
        """
        if not clips:
            return clips
        
        # é–‹å§‹æ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
        clips.sort(key=lambda x: x['start_time'])
        
        # æœ€å°è¡¨ç¤ºæ™‚é–“ã‚’ç¢ºä¿
        for clip in clips:
            duration = clip['end_time'] - clip['start_time']
            if duration < self.min_duration:
                clip['end_time'] = clip['start_time'] + self.min_duration
        
        # æœ€å¤§è¡¨ç¤ºæ™‚é–“ã‚’åˆ¶é™ï¼ˆæœ€å¾Œã®ã‚¯ãƒªãƒƒãƒ—ã¯é™¤å¤–ï¼‰
        for i, clip in enumerate(clips):
            duration = clip['end_time'] - clip['start_time']
            is_last_clip = (i == len(clips) - 1)
            if duration > self.max_duration and not is_last_clip:
                clip['end_time'] = clip['start_time'] + self.max_duration
        
        # ã€é‡è¦ã€‘éš™é–“ã®å¼·åˆ¶çµåˆ: å‰ã®ç”»åƒã‚’å»¶é•·ã—ã¦éš™é–“ã‚’åŸ‹ã‚ã‚‹
        result = []
        for i, clip in enumerate(clips):
            if i == 0:
                # æœ€åˆã®ã‚¯ãƒªãƒƒãƒ—: section_startã‹ã‚‰é–‹å§‹ã™ã‚‹ã‚ˆã†ã«èª¿æ•´
                if clip['start_time'] > section_start:
                    # é–‹å§‹æ™‚é–“ã‚’å‰ã«ä¼¸ã°ã™
                    clip['start_time'] = section_start
                result.append(clip)
            else:
                prev_clip = result[-1]
                gap = clip['start_time'] - prev_clip['end_time']
                
                if gap > 0:
                    # éš™é–“ãŒã‚ã‚‹å ´åˆ: å‰ã®ç”»åƒã‚’å»¶é•·ã—ã¦åŸ‹ã‚ã‚‹
                    self.logger.debug(
                        f"Filling gap of {gap:.3f}s by extending previous image "
                        f"({prev_clip['start_time']:.3f}s - {prev_clip['end_time']:.3f}s -> "
                        f"{prev_clip['start_time']:.3f}s - {clip['start_time']:.3f}s)"
                    )
                    prev_clip['end_time'] = clip['start_time']
                elif gap < 0:
                    # é‡è¤‡ã—ã¦ã„ã‚‹å ´åˆ: å‰ã®ã‚¯ãƒªãƒƒãƒ—ã‚’å»¶é•·ï¼ˆå¾Œã®ã‚¯ãƒªãƒƒãƒ—ã®çµ‚äº†æ™‚é–“ã¾ã§ï¼‰
                    if clip['end_time'] > prev_clip['end_time']:
                        prev_clip['end_time'] = clip['end_time']
                    # é‡è¤‡ã—ã¦ã„ã‚‹ã‚¯ãƒªãƒƒãƒ—ã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                result.append(clip)
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨åŸŸã®å®Œå…¨ã‚«ãƒãƒ¼
        if result:
            # æœ€åˆã®ã‚¯ãƒªãƒƒãƒ—ã‚’section_startã‹ã‚‰é–‹å§‹
            if result[0]['start_time'] > section_start:
                result[0]['start_time'] = section_start
            
            # æœ€å¾Œã®ã‚¯ãƒªãƒƒãƒ—ã‚’section_endã¾ã§å»¶é•·
            last_clip = result[-1]
            if last_clip['end_time'] < section_end:
                self.logger.debug(
                    f"Extending last clip to section end: "
                    f"{last_clip['end_time']:.3f}s -> {section_end:.3f}s"
                )
                last_clip['end_time'] = section_end
            elif last_clip['end_time'] > section_end:
                last_clip['end_time'] = section_end
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯: é€£ç¶šæ€§ã®æ¤œè¨¼
        for i in range(len(result) - 1):
            current_end = result[i]['end_time']
            next_start = result[i + 1]['start_time']
            gap = next_start - current_end
            if abs(gap) > 0.001:  # 1ãƒŸãƒªç§’ä»¥ä¸Šã®éš™é–“ã¯è¨±å®¹ã—ãªã„
                self.logger.warning(
                    f"Warning: Gap detected between clips {i} and {i+1}: {gap:.6f}s. "
                    f"Extending previous clip to fill."
                )
                result[i]['end_time'] = next_start
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨åŸŸã®ã‚«ãƒãƒ¼ç¢ºèª
        if result:
            first_start = result[0]['start_time']
            last_end = result[-1]['end_time']
            if abs(first_start - section_start) > 0.001:
                self.logger.warning(
                    f"First clip does not start at section_start: "
                    f"{first_start:.6f}s != {section_start:.6f}s"
                )
                result[0]['start_time'] = section_start
            if abs(last_end - section_end) > 0.001:
                self.logger.warning(
                    f"Last clip does not end at section_end: "
                    f"{last_end:.6f}s != {section_end:.6f}s"
                )
                result[-1]['end_time'] = section_end
        
        return result
    
    def _load_section_boundaries(self) -> Dict[int, Tuple[float, float]]:
        """
        audio_timing.jsonã‹ã‚‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³å¢ƒç•Œã‚’èª­ã¿è¾¼ã‚€
        
        Returns:
            ã‚»ã‚¯ã‚·ãƒ§ãƒ³ID -> (é–‹å§‹æ™‚é–“, çµ‚äº†æ™‚é–“) ã®è¾æ›¸
        """
        boundaries = {}
        audio_timing_path = self.working_dir / "02_audio" / "audio_timing.json"
        
        if not audio_timing_path.exists():
            self.logger.warning(f"audio_timing.json not found: {audio_timing_path}")
            return boundaries
        
        try:
            with open(audio_timing_path, 'r', encoding='utf-8') as f:
                audio_timing = json.load(f)
            
            cumulative_time = 0.0
            
            if isinstance(audio_timing, list):
                sections = audio_timing
            elif isinstance(audio_timing, dict):
                sections = audio_timing.get('sections', [audio_timing])
            else:
                self.logger.warning(f"Unexpected audio_timing format: {type(audio_timing)}")
                return boundaries
            
            for section in sections:
                section_id = section.get('section_id')
                char_end_times = section.get('char_end_times', [])
                
                if section_id and char_end_times:
                    section_duration = char_end_times[-1]
                    boundaries[section_id] = (cumulative_time, cumulative_time + section_duration)
                    cumulative_time += section_duration
            
        except Exception as e:
            self.logger.error(f"Failed to load section boundaries: {e}", exc_info=True)
        
        return boundaries
    
    def _get_subtitle_section(
        self,
        subtitle: dict,
        script_data: dict
    ) -> int:
        """
        å­—å¹•ãŒå±ã™ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—
        
        Args:
            subtitle: å­—å¹•ãƒ‡ãƒ¼ã‚¿
            script_data: å°æœ¬ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ã‚»ã‚¯ã‚·ãƒ§ãƒ³ID
        """
        start_time = subtitle.get('start_time', 0.0)
        boundaries = self._load_section_boundaries()
        
        for section_id, (section_start, section_end) in boundaries.items():
            if section_start <= start_time < section_end:
                return section_id
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: script.jsonã‹ã‚‰åˆ¤å®š
        sections = script_data.get('sections', [])
        cumulative_time = 0.0
        
        for section in sections:
            section_duration = section.get('estimated_duration', 0.0)
            if cumulative_time <= start_time < cumulative_time + section_duration:
                return section.get('section_id', 1)
            cumulative_time += section_duration
        
        return sections[0].get('section_id', 1) if sections else 1
    
    def _get_section_images(
        self,
        classified_images: dict,
        section_id: int
    ) -> List[dict]:
        """
        ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ç”»åƒã‚’å–å¾—
        
        Args:
            classified_images: åˆ†é¡æ¸ˆã¿ç”»åƒãƒ‡ãƒ¼ã‚¿
            section_id: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ID
            
        Returns:
            ç”»åƒãƒªã‚¹ãƒˆ
        """
        all_images = classified_images.get('images', [])
        section_images = []
        
        for image in all_images:
            file_path = Path(image.get('file_path', ''))
            image_section = self._get_image_section(file_path)
            
            if image_section == section_id:
                section_images.append(image)
        
        return section_images
    
    def _get_image_section(self, image_path: Path) -> int:
        """
        ç”»åƒãŒå±ã™ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ï¼‰
        
        Args:
            image_path: ç”»åƒãƒ‘ã‚¹
            
        Returns:
            ã‚»ã‚¯ã‚·ãƒ§ãƒ³ID
        """
        import re
        filename = image_path.name
        match = re.search(r'section_(\d+)', filename)
        if match:
            return int(match.group(1))
        return 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _create_fallback_clips(
        self,
        section_images: List[dict],
        section_start: float,
        section_end: float
    ) -> List[Dict[str, Any]]:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆï¼ˆå‡ç­‰åˆ†å‰²ï¼‰
        
        Args:
            section_images: ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ç”»åƒãƒªã‚¹ãƒˆ
            section_start: ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹æ™‚é–“
            section_end: ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚é–“
            
        Returns:
            ç”»åƒã‚¯ãƒªãƒƒãƒ—ã®ãƒªã‚¹ãƒˆ
        """
        if not section_images:
            return []
        
        num_images = len(section_images)
        duration_per_image = (section_end - section_start) / num_images
        fallback_clips = []
        current_time = section_start
        
        for image_data in section_images:
            fallback_clips.append({
                'image_path': str(Path(image_data.get('file_path', ''))),
                'start_time': current_time,
                'end_time': current_time + duration_per_image,
                'keyword_matched': None,
                'confidence': 0.0,
                'match_type': 'fallback_equal_split'
            })
            current_time += duration_per_image
        
        return fallback_clips

