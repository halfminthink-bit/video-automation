# phase: audio generation
# ========================================
# Phase 2: 音声生成設定
# ========================================

# テキスト最適化設定
text_optimization:
  enabled: false                         # ⭐ Kokoro TTSは漢字を正確に読めるため無効化
  # Kokoro TTSはpyopenjtalkで漢字→音素変換が正確
  # 平仮名変換のオーバーヘッド（Claude API呼び出し）が不要
  # 元のnarrationテキストをそのまま使用することで：
  # - 処理速度が向上
  # - Claude APIコストが削減
  # - 字幕表示も元のテキストを使用
  model: "claude-sonnet-4-20250514"      # 使用するClaudeモデル（enabledがfalseの場合は未使用）
  use_context: true                      # 全体の文脈情報を活用（enabledがfalseの場合は未使用）

# 文脈制御設定
context_awareness:
  use_previous_text: true                # 前のセクションを文脈に使用
  use_next_text: true                    # 次のセクションを文脈に使用

# ========================================
# 音声生成サービス選択
# ========================================
# "kokoro": Kokoro TTS FastAPI（完全無料、タイムスタンプ対応）
# "elevenlabs": ElevenLabs API（高品質、有料）
# "azure": Azure AI Speech（高品質、日本語精度高、有料）
service: "kokoro"

# ========================================
# 句点での間隔制御
# ========================================
punctuation_pause:
  enabled: true                    # 句点での間隔制御を有効化

  # 各句読点の後に挿入する無音時間（秒）
  pause_duration:
    period: 0.8                    # 「。」の後
    exclamation: 0.9               # 「！」の後
    question: 0.9                  # 「？」の後
    comma: 0.0                     # 「、」の後（通常は挿入しない）

  # セクション末尾の句点は間隔を挿入しない
  skip_section_end: true

# ========================================
# Kokoro TTS 設定（service: "kokoro" の場合）
# ========================================
kokoro:
  # APIのベースURL（環境変数 KOKORO_API_URL で上書き可能）
  api_url: "http://localhost:8880"

  # 使用する音声名
  # 利用可能な日本語の声（Japanese Female）:
  # - jf_alpha ⭐推奨（日本語女性音声、自然）
  # 利用可能な英語の声（American Female）:
  # - af_bella （人気、安定）
  # - af_sarah （人気、自然）
  # - af_sky （明るめ）
  # - af_heart （落ち着き）
  # - af_alloy, af_aoede, af_jessica, af_kore, af_nicole, af_nova, af_river
  voice: "jf_alpha"

  # 速度（0.5-2.0）
  speed: 1.1

  # 出力フォーマット
  response_format: "mp3"  # mp3, wav, opus, flac

# ========================================
# Azure AI Speech 設定（service: "azure" の場合）
# ========================================
azure:
  # APIキー（環境変数 AZURE_SPEECH_KEY で上書き可能）
  # .envファイルに AZURE_SPEECH_KEY=your_key_here を設定してください
  api_key: null  # .envから自動取得

  # リージョン（環境変数 AZURE_SPEECH_REGION で上書き可能）
  region: "japaneast"  # 日本語に最適

  # 使用する音声名
  # 日本語女性音声: ja-JP-NanamiNeural（推奨）, ja-JP-MayuNeural, ja-JP-AoiNeural, ja-JP-ShioriNeural
  # 日本語男性音声: ja-JP-NaokiNeural（推奨）, ja-JP-KeitaNeural, ja-JP-DaichiNeural
  voice_name: "ja-JP-NanamiNeural"  # デフォルト女性音声

  # 感情スタイル（オプション）
  # cheerful（明るい）, sad（悲しい）, angry（怒り）, fearful（恐れ）,
  # disgruntled（不満）, serious（真面目）, friendly（親しみやすい）, hopeful（希望に満ちた）
  style: null  # スタイルなし

  # 速度（0.5-2.0）
  speed: 1.0

  # ピッチ（-50% ～ +50%）
  pitch: "+0%"

  # 出力フォーマット設定
  output_format: "Audio16Khz32KBitRateMonoMp3"  # MP3形式

# ========================================
# タイミング抽出設定
# ========================================
# 🔥 ElevenLabs Forced Alignment（台本と音声の完璧なアラインメント）
# ElevenLabs FAを使用することで、固有名詞も含め100%正確なタイミングを取得
use_elevenlabs_fa: true

# ElevenLabs API Key（環境変数 ELEVENLABS_API_KEY から自動取得）
# .envファイルに ELEVENLABS_API_KEY=your_key_here を設定してください
# 未設定の場合は自動的にWhisperにフォールバックします
# 注: この値は参照用です。実際には環境変数から直接読み込まれます
elevenlabs_api_key: null

# ========================================
# Whisper タイムスタンプ取得設定（フォールバック用）
# ========================================
# ElevenLabs FAが失敗した場合、またはuse_elevenlabs_fa=falseの場合に使用
whisper:
  enabled: true           # Whisper使用の有効化
  model: "small"         # tiny, base, small, medium, large
                         # small: 約500MB、高精度（日本語認識精度向上のため推奨）
                         # base: 約150MB、精度と速度のバランスが良い
                         # tiny: 約75MB、高速だが精度低め
  language: "ja"         # 日本語
  device: "auto"         # auto（自動検出）, cuda（GPU）, cpu

  # 🔥 stable-ts設定（音声と字幕の完璧な同期のため）
  use_stable_ts: true            # stable-tsを使用（無音抑制、ギャップ調整が有効）
  suppress_silence: true         # 無音区間を抑制（字幕の早送り感を解消）
  vad: true                      # Voice Activity Detection有効化
  vad_threshold: 0.35            # VAD閾値（0-1、低いほど厳格）

  # タイミング精度向上のための追加設定
  # これらは whisper_timing.py 内で condition_on_previous_text=False として実装済み
  # 累積エラー防止のため、前のテキストに依存しない設定を使用

# ========================================
# ElevenLabs API設定（service: "elevenlabs" の場合）
# ========================================
# 日本語対応のvoice_id（以下から選択）:
# - "pNInz6obpgDQGcFmaJgB": Adam (多言語対応、男性)
# - "onwK4e9ZLuTAKqWW03F9": Daniel (多言語対応、男性)
# または、ElevenLabsダッシュボードで日本語対応の音声を確認してください
voice_id: "3JDquces8E8bkmvbh6Bc"  
model: "eleven_turbo_v2_5"  
with_timestamps: true  # タイムスタンプ付き音声生成を使用

# 音声品質設定
settings:
  stability: 0.7          # 0-1（低いほどバリエーション豊か）日本語は少し高めが良い
  similarity_boost: 0.75  # 0-1（高いほど元の声に近い）
  style: 0                # 0-1（スタイルの強さ）
  use_speaker_boost: true # スピーカーブースト有効化
  speed: 1.0              # 0.25-4.0（1.0が通常速度）

# 出力フォーマット
format:
  codec: "mp3_44100_128"  # mp3_44100_128 or mp3_44100_192
  sample_rate: 44100
  channels: 1  # モノラル

# セクション間の無音時間（秒）
inter_section_silence: 0.5

# リトライ設定
retry:
  max_attempts: 5
  delay_seconds: 10  # レート制限を考慮

# キャッシュ設定
cache:
  enabled: true
  use_cached_audio: true  # 同じナレーションならキャッシュを使用

# ========================================
# セクションタイトル設定
# ========================================
section_title:
  enabled: true

  # 読み上げ速度（0.8倍速）
  speed: 0.8

  # タイトル後の無音時間（秒）
  silence_after: 2.0